# ==========================================
# LLM Configuration
# ==========================================

# -------- LOCAL OLLAMA SETUP (Default) --------
# Use this configuration for local Ollama models
USE_REMOTE_SERVER=false
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5vl:3b

# -------- REMOTE GOOGLE COLAB SERVER SETUP --------
# Use this configuration to connect to the Google Colab server
# 1. Run the Colab notebook script
# 2. Copy the ngrok public URL (e.g., https://xxxx-xx-xxx-xxx-xx.ngrok-free.app)
# 3. Set USE_REMOTE_SERVER=true
# 4. Paste the ngrok URL as REMOTE_SERVER_URL

# USE_REMOTE_SERVER=true
# REMOTE_SERVER_URL=https://your-ngrok-url.ngrok-free.app

# ==========================================
# INSTRUCTIONS TO SWITCH TO REMOTE SERVER
# ==========================================
# 1. Comment out or set USE_REMOTE_SERVER=false for local Ollama
# 2. Uncomment USE_REMOTE_SERVER=true and REMOTE_SERVER_URL
# 3. Replace "your-ngrok-url" with the actual ngrok URL from Colab
# 4. Save this file as ".env" (remove .example)
# 5. Restart your application
